{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ivychen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/project2/cmsc25025/wikipedia/wiki-text.txt', 'r') as f:\n",
    "    wiki = f.read()\n",
    "    words = \"\".join(wiki)\n",
    "    words = words.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124301827"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the word frequency excluding the stop words\n",
    "fdist = FreqDist()\n",
    "for word in words:\n",
    "    if word.lower() not in stop_words:\n",
    "        fdist[word.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear less than n times \n",
    "def remove_words(n):\n",
    "    V = set()\n",
    "    for word in fdist:\n",
    "        if fdist[word] >= n:\n",
    "            V.add(word)\n",
    "    return list(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expriment with n for several times, if n = 400, we \n",
    "# have the number of words in the vocabulary roughly \n",
    "# equal to 15,000 (15, 348). V: key: word, value: ind\n",
    "V = remove_words(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of texts to expriment first \n",
    "# remove empty string\n",
    "words = [words[i] for i in range(len(words)) if words[i] != \"\"]\n",
    "# sub_words = words[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PMI \n",
    "# Define S^P and N^p(w,c) with a window of size 5 \n",
    "import json\n",
    "from collections import defaultdict\n",
    "def create_sp(words):\n",
    "    # create a default dictionary dict: word- context- frequency\n",
    "    word_context = defaultdict(lambda: defaultdict(int))\n",
    "    length = len(words)\n",
    "    for i in range(length):\n",
    "        # track \n",
    "        if i%1000000 == 0:\n",
    "            print(i//1000000) \n",
    "            \n",
    "        if i%3000000 == 0:\n",
    "            with open('W{}'.format(i), 'w') as f:\n",
    "                json.dump(word_context, f)\n",
    "            \n",
    "        word = words[i]\n",
    "        if word in V:\n",
    "            if i <= 4:\n",
    "                context = [words[j] for j in range(-(5-i), (i+6)) if j !=i]\n",
    "            elif i >= length-5:\n",
    "                context = [words[j] for j in range(-(5-i), length) if j !=i]\n",
    "                context += [words[j] for j in range(i+6-length)]\n",
    "            else:\n",
    "                context = [words[i+j] for j in range(-5, 6) if j !=0]\n",
    "            for c in context:\n",
    "                word_context[word][c] += 1\n",
    "    return word_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = create_sp(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "#with open('W', 'w') as f:\n",
    "#    json.dump(wc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('W','r') as f:\n",
    "    wc1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate N^p and N^c\n",
    "def compute_np(word_context):\n",
    "    wset, cset = defaultdict(int), defaultdict(int)\n",
    "    for w, d in word_context.items():\n",
    "        wset[w] = sum(d.values())\n",
    "        for c, f in d.items():\n",
    "            cset[c] += f\n",
    "    return wset, cset       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_w(vocab, word_con_pair):\n",
    "    n = len(vocab)\n",
    "    M = np.zeros([n,n])\n",
    "    wset, cset = compute_np(word_con_pair)\n",
    "    for i in range(n):\n",
    "        if i%1000 == 0:\n",
    "            print(i//1000)         \n",
    "        for j in range(n):\n",
    "            w, c = vocab[i], vocab[j]\n",
    "            nij = word_con_pair[w][c]\n",
    "            ni = wset[w] \n",
    "            nj = cset[c]\n",
    "            M[i, j] = np.log( (nij+1)/(ni*nj) ) \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = create_w(V, wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('M', M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create word embeddings \n",
    "import scipy\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix\n",
    "u, s, vt = svds(csr_matrix(M), k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = u.dot(np.diag(np.sqrt(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialization and store\n",
    "with open('Wmat.pickle', 'wb') as handle:\n",
    "    pickle.dump(W, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# with open('Wmat.pickle', 'rb') as handle:\n",
    "#    b = pickle.load(handle)\n",
    "# print(W == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics:  7704\n",
      "republican:  7043\n",
      "einstein:  9162\n",
      "algebra:  3429\n",
      "fish:  14517\n"
     ]
    }
   ],
   "source": [
    "# find the 5 closest words in the embedding space: physics, republican, einstein, algebra, fish\n",
    "print(\"physics: \", V.index('physics'))\n",
    "print(\"republican: \", V.index('republican'))\n",
    "print(\"einstein: \", V.index('einstein'))\n",
    "print(\"algebra: \", V.index('algebra'))\n",
    "print(\"fish: \", V.index('fish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm as norm\n",
    "def fiveClosest(word, V, W):\n",
    "    wind = V.index(word)\n",
    "    ind = np.argsort([norm(W[i] - W[wind]) for i in range(len(W))])[1:6]\n",
    "    return [V[i] for i in ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mechanics', 'chemistry', 'quantum', 'mathematics', 'theoretical']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveClosest('physics', V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['senator', 'democrat', 'democrats', 'representative', 'candidate']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveClosest('republican', V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relativity', 'paradox', 'planck', 'maxwell', 'leibniz']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveClosest('einstein', V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algebraic', 'finite', 'theorem', 'topology', 'calculus']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveClosest('algebra', V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit', 'meat', 'eggs', 'cattle', 'wild']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveClosest('fish', V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find analogies v= v_democrat - v_repub + v_conserv\n",
    "def findAnalogies(w1, w2, w3, V, W):\n",
    "    wi1, wi2, wi3 = V.index(w1), V.index(w2), V.index(w3)\n",
    "    ana = W[wi1] - W[wi2] + W[wi3] \n",
    "    return [V[j] for j in np.argsort([norm(W[i]-ana) for i in range(len(W))])[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conservative', 'liberal', 'opposition', 'democratic', 'leaders']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findAnalogies('republican', 'democrat', 'conservative', V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
